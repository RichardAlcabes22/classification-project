{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fc6a69d",
   "metadata": {},
   "source": [
    "## TELCO CUSTOMER BASE (TCB) CHURN: An Initial Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b37652",
   "metadata": {},
   "source": [
    "#### WORKING PREMISE:\n",
    "\n",
    "**While each Customer will have their own particular reason for parting ways with Telco, the two key ingredients needed for Customer churn are:**\n",
    "1. Low or No \"Perception of Commitment\" to Telco.\n",
    "2. Minimal Switching Costs associated with leaving Telco.\n",
    "\n",
    "#### The goals of this initial exploration are as follows:\n",
    "- Discover the \"Driving Factors\" which contribute most to Customer churn.\n",
    "- Discover the Customer segment which is most likely to churn.\n",
    "- Discover the Customer segment which is most responsible for revenue.\n",
    "- Identify those Customers who exhibit the characteristics of said target Customer segment.\n",
    "- Create a Machine Learning Model which can accurately predict Customer churn with greater accuracy than baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3456d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DS Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Data Acquisition\n",
    "from pydataset import data\n",
    "import env\n",
    "import acquire as acq\n",
    "import prepare as prp\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# scikit learn submodules\n",
    "from sklearn.tree import DecisionTreeClassifier, export_text, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadecf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's Acquire and Prepare the DataFrame\n",
    "# load telco via acquire.py (be sure to place acquire.py in same directory as this Notebook)\n",
    "df = acq.new_telco_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4923fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the prep_telco function in prepare.py, we will drop unneeded columns and encode others\n",
    "df = prp.prep_telco(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33593ca7",
   "metadata": {},
   "source": [
    "#### Initial questions and hypothesis:\n",
    "#### What % of Telco Customer Base (TCB) has No \"Perception of  Commitment\" to Telco in combination with low \"Switching Costs\"?\n",
    "\n",
    "- Do customers with **\"Month-to-Month\"** contracts churn out at a statistically significant greater rate than the overall population?\n",
    "- Do customers with **\"Paperless Billing\"** enabled churn out at a statistically significant greater rate than the overall population?\n",
    "- Do customers **without \"Dependents\"** churn out at a statistically significant greater rate than the overall population?\n",
    "- Do customers who comprise the largest customer segment churn out at a statistically significant greater rate than the overall population?\n",
    "\n",
    "#### HYPOTHESIS: Customers in these segmnents churn at greater rates than the overall Telco Customer Base (TCB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06fc299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using prepare.py, split the Dataset into TRAIN(.56), VALIDATE(.24), and TEST(.20) subsets:\n",
    "train, validate, test = prp.split_data(df,'churn')\n",
    "# quick check of the numbers for each should be 3943-1691-1409\n",
    "train.shape[0],validate.shape[0],test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3ac5e4",
   "metadata": {},
   "source": [
    "#### Technical disclaimer...\n",
    "\n",
    "- The original dataset contained 7043 records.  Due to the unique req's for building a ML model that actually functions properly with UNSEEN data, we have split the data into several subsets, for Training the model and for Testing the model.  \n",
    "- Therefore, note that our univariate analyses will imply 3943 total records instead.  The conclusions gained from the analysis will be just as valid as if we had used the full dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1271606e",
   "metadata": {},
   "source": [
    "### Let us now compare Customer Counts in a few specific Categories:\n",
    "\n",
    "- **70%** of TCB has **NO DEPENDENTS** on Telco Data/Voice Plans  \n",
    "- **59%** of TCB has **PAPERLESS BILLING** on Telco Data/Voice Plans \n",
    "- **54%** of TCB has **MONTH-to-MONTH** contracts on Telco Data/Voice Plans  \n",
    "\n",
    "### Taken in combination, this represents 22.3 % of TCB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da3881f",
   "metadata": {},
   "source": [
    "### What are the total counts of TCB by DEPENDENTS status?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001c7b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chart showing raw counts of customers with dependents and customers without:\n",
    "print(f'TCB with NO DEPENDENTS:')\n",
    "sns.countplot(data=train, x='dependents')\n",
    "plt.show()\n",
    "print(\n",
    "    pd.concat([train['dependents'].value_counts(),\n",
    "    train['dependents'].value_counts(normalize=True)],\n",
    "         axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052e8a35",
   "metadata": {},
   "source": [
    "### TAKEAWAY:  70% of TCB does not have to consider the Switching Costs associated with transitioning DEPENDENTS to a new service provider."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6eb2adf",
   "metadata": {},
   "source": [
    "### What are the total counts of TCB by PAPERLESS BILLING status?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a764946b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chart showing raw counts of customers with paperless_billing and customers without:\n",
    "print(f'TCB with PAPERLESS BILLING:')\n",
    "sns.countplot(data=train, x='paperless_billing')\n",
    "plt.show()\n",
    "print(\n",
    "    pd.concat([train['paperless_billing'].value_counts(),\n",
    "    train['paperless_billing'].value_counts(normalize=True)],\n",
    "         axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdb1a5d",
   "metadata": {},
   "source": [
    "### TAKEAWAY:  59% of TCB never sees a monthly paper bill.  This may reduce some Customers \"Perception of Commitment\"...  Out of Sight, Out of Mind..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06979fe6",
   "metadata": {},
   "source": [
    "### What are the total counts of TCB by CONTRACT TYPE?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c81107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chart showing raw counts of customers by contract type:\n",
    "print(f'TCB with MONTH-to_MONTH contract:')\n",
    "sns.countplot(data=train, x='contract_type')\n",
    "plt.show()\n",
    "print(\n",
    "    pd.concat([train['contract_type'].value_counts(),\n",
    "    train['contract_type'].value_counts(normalize=True)],\n",
    "         axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e842412",
   "metadata": {},
   "source": [
    "### TAKEAWAY:  54% of TCB does not have to consider monetary Switching Costs associated with breaking an extended contract.  Instead, they can churn-out at their convenience.\n",
    "\n",
    "# BOTTOM LINE  \\#1: 1 in 5 Customers exhibit ALL of these characteristics and an unmeasured plurality of Customers fall into at least one of these categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b910867b",
   "metadata": {},
   "source": [
    "--------\n",
    "--------\n",
    "--------\n",
    "\n",
    "### Let us now compare Customer Churn Rates in a few specific Categories and compare them to the overall TCB mean average:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c04305",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Chart showing churn rate of customers by dependent status, as compared to overall churn rate:\n",
    "sns.barplot(data=train,\n",
    "            x='dependents',\n",
    "            y = 'churn_Yes',\n",
    "           ci=False)\n",
    "plt.title(f'Customer CHURN Rate based on DEPENDENTS')\n",
    "plt.axhline(train.churn_Yes.mean(),label='Overall TCB Churn Rate: ~ 26%',color='red',linewidth=4)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77353975",
   "metadata": {},
   "source": [
    "### TAKEAWAY:  With the Overall TCB Churn Rate at ~ 26%, Customers WITHOUT DEPENDENTS churn-out at a sample rate of ~ 31% .\n",
    "\n",
    "### This chart is nice, however how can we be sure that these results are not just due to random sampling?  How statistically significant is this result?\n",
    "#### Glad you asked, let's run a quick Chi$^2$ test for Independence to find out how likely it is that we would have obtained these results via random sampling..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94976e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets Run a Chi2 Test to find out how likely it is that we would get this result by random sampling:\n",
    "\n",
    "# H0: No statistically significant difference exists between the churn rates of customers with DEPENDENTS \n",
    "#     and ALL customers. ie the two samples were drawn from the same population.\n",
    "    \n",
    "# H1: A statistically significant difference exists between the churn rates of customers with DEPENDENTS \n",
    "#     and ALL customers.  ie the two samples were drawn from differing populations.\n",
    "\n",
    "df_dependents_x =pd.crosstab(train['dependents'],train['churn'])\n",
    "df_dependents_x\n",
    "stats.chi2_contingency(df_dependents_x)\n",
    "\n",
    "# This function has 4 outputs: Chi2 Coefficient (94.41...), p-value (2.55 e-22)(think...very close to ZERO)),\n",
    "# the number of Degrees of Freedom (1) and a 2-dim array showing the expected frequency of each case assuming that\n",
    "# null hypothesis were True\n",
    "# With a p-value < 0.05, we can REJECT the NULL hypothesis.  There is evidence to suggest that a statistically\n",
    "# significant difference exists between the two rates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dda68ed",
   "metadata": {},
   "source": [
    "### TAKEAWAY:  With a p-value < 0.05, we can REJECT the NULL hypothesis.  There is evidence to suggest that a statistically significant difference exists between the two rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b419b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Chart showing churn rate of customers by paperless status, as compared to overall churn rate:\n",
    "sns.barplot(data=train,\n",
    "            x='paperless_billing',\n",
    "            y = 'churn_Yes',\n",
    "           ci=False)\n",
    "plt.title(f'Customer CHURN Rate based on PAPERLESS')\n",
    "plt.axhline(train.churn_Yes.mean(),label='Overall TCB Churn Rate: ~ 26%',color='red',linewidth=4)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f9ca79",
   "metadata": {},
   "source": [
    "### TAKEAWAY:  With the Overall TCB Churn Rate at ~ 26%, Customers WITH PAPERLESS BILLING churn-out at a sample rate of ~ 33% .\n",
    "\n",
    "### Again, how statistically significant is this result?\n",
    "#### Let's run a quick Chi$^2$ test for Independence to find out how likely it is that we would have obtained these results via random sampling..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade18660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets Run a Chi2 Test to find out how likely it is that we would get this result by random sampling:\n",
    "\n",
    "# H0: No statistically significant difference exists between the churn rates of customers with DEPENDENTS \n",
    "#     and ALL customers. ie the two samples were drawn from the same population.\n",
    "    \n",
    "# H1: A statistically significant difference exists between the churn rates of customers with DEPENDENTS \n",
    "#     and ALL customers.  ie the two samples were drawn from differing populations.\n",
    "\n",
    "df_dependents_x =pd.crosstab(train['paperless_billing'],train['churn'])\n",
    "df_dependents_x\n",
    "stats.chi2_contingency(df_dependents_x)\n",
    "\n",
    "# This function has 4 outputs: Chi2 Coefficient (131.66...), p-value (1.77 e-30)(think...very close to ZERO)),\n",
    "# the number of Degrees of Freedom (1) and a 2-dim array showing the expected frequency of each case assuming that\n",
    "# null hypothesis were True\n",
    "# With a p-value < 0.05, we can REJECT the NULL hypothesis.  There is evidence to suggest that a statistically\n",
    "# significant difference exists between the two rates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c3f3ee",
   "metadata": {},
   "source": [
    "### TAKEAWAY:  With a p-value < 0.05, we can REJECT the NULL hypothesis.  There is evidence to suggest that a statistically significant difference exists between the two rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bc8b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chart showing churn rate of customers by contract_type, as compared to overall churn rate:\n",
    "sns.barplot(data=train,\n",
    "            x='contract_type',\n",
    "            y = 'churn_Yes',\n",
    "           ci=False)\n",
    "plt.title(f'Customer CHURN Rate based on CONTRACT TYPE')\n",
    "plt.axhline(train.churn_Yes.mean(),label='Overall TCB Churn Rate: ~ 26%',color='red',linewidth=4)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f70840",
   "metadata": {},
   "source": [
    "### TAKEAWAY:  With the Overall TCB Churn Rate at ~ 26%, Customers with MONTH-TO-MONTH contracts churn-out at a sample rate of ~ 43% .\n",
    "\n",
    "### How statistically significant is this result?\n",
    "#### A Chi$^2$ test for Independence to find out how likely it is that we would have obtained these results via random sampling..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdf8bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets Run a Chi2 Test to find out how likely it is that we would get this result by random sampling:\n",
    "\n",
    "# H0: No statistically significant difference exists between the churn rates of customers with DEPENDENTS \n",
    "#     and ALL customers. ie the two samples were drawn from the same population.\n",
    "    \n",
    "# H1: A statistically significant difference exists between the churn rates of customers with DEPENDENTS \n",
    "#     and ALL customers.  ie the two samples were drawn from differing populations.\n",
    "\n",
    "df_dependents_x =pd.crosstab(train['contract_type'],train['churn'])\n",
    "df_dependents_x\n",
    "stats.chi2_contingency(df_dependents_x)\n",
    "\n",
    "# This function has 4 outputs: Chi2 Coefficient (650.61...), p-value (2.25 e-142)(think...very close to ZERO)),\n",
    "# the number of Degrees of Freedom (2) and a 2-dim array showing the expected frequency of each case assuming that\n",
    "# null hypothesis were True\n",
    "# With a p-value < 0.05, we can REJECT the NULL hypothesis.  There is evidence to suggest that a statistically\n",
    "# significant difference exists between the two rates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d50cfe4",
   "metadata": {},
   "source": [
    "### TAKEAWAY:  With a p-value < 0.05, we can REJECT the NULL hypothesis.  There is evidence to suggest that a statistically significant difference exists between the two rates.\n",
    "\n",
    "# BOTTOM LINE  \\#2: ALL Customers who exist in these categories exhibit a CHURN Rate Greater than the TCB mean average rate.\n",
    "\n",
    "# 1 + 2 = ...\n",
    "# This situation must be addressed...the largest segment of our TCB also Churns out at the greatest rate.\n",
    "\n",
    "----\n",
    "----\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8238b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We divide the columns into categorical, categorical_encoded, and numerical\n",
    "\n",
    "cat_cols, cat_cols_e, num_cols = [], [], []\n",
    "for col in train.columns:\n",
    "    if train[col].dtype == 'O':\n",
    "        cat_cols.append(col)\n",
    "    else:\n",
    "        if train[col].nunique() < 10:\n",
    "            cat_cols_e.append(col)\n",
    "        else:\n",
    "            num_cols.append(col)\n",
    "            \n",
    "# We then assign the needed columns to variables and then affix those column groupings to the appropriate\n",
    "# TRAIN, VALIDATE, and TEST subsets\n",
    "X_cols = cat_cols_e\n",
    "y_cols = 'churn_Yes'\n",
    "X_cols.remove('churn_Yes')\n",
    "X_cols,y_cols\n",
    "\n",
    "X_train = train[X_cols]\n",
    "y_train = train['churn']\n",
    "\n",
    "X_validate = validate[X_cols]\n",
    "y_validate = validate['churn']\n",
    "\n",
    "X_test = test[X_cols]\n",
    "y_test = test['churn']\n",
    "\n",
    "# create the Decision Tree Classifier model \n",
    "clf = DecisionTreeClassifier()\n",
    "# fit the model to the TRAIN dataset:\n",
    "clf.fit(X_train, y_train)\n",
    "# use the model by calling for the predictions made via the TRAIN dataset\n",
    "model_a_preds = clf.predict(X_train)\n",
    "pd.crosstab(model_a_preds,y_train) # a confusion matrix with ACTUALS as columns and PREDICTIONS as rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f683a727",
   "metadata": {},
   "source": [
    "### GIVEN THE FOLLOWING:\n",
    "- Customizable options provided by Telco is the reason we have our Customer Base.\n",
    "- Removing Month-to-Month contracts or any other variable which may appear to be correlated with high churn rates is \"bad business\" and simply not feasible.\n",
    "- Accepting significantly higher Churn rates in certain Customer Segments as just \"the cost of doing business\" is not an appropriately proactive approach.\n",
    "- Providing \"loyalty incentives\" to all customers regardless of their propensity to Churnis not economically viable for long-term growth.\n",
    "\n",
    "# BOTTOM LINE  \\#3: We must find a way to IDENTIFY those Customers who are most likely to Churn and PROVIDE Targeted \"Loyalty Incentives\" designed for Customer retention. \n",
    "\n",
    "## The good news is that we have a prototype Model that is being tested exactly for this puprose!\n",
    "\n",
    "# 1 + 2 = 3\n",
    "\n",
    "----\n",
    "----\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ed4e80",
   "metadata": {},
   "source": [
    "### LET\"S FIRST ESTABLISH OUR BASELINE:\n",
    "- Current Overall TCB Churn rate is ~26.5%.\n",
    "- Therefore, 73.5% of TCB has no current propensity to churn. CALL THIS BASELINE ACCURACY (73.5%)\n",
    "- If we assume all Customers Will Not Churn...we will be wrong in 26.5% of all cases.\n",
    "- This is the current state of affairs at Telco, and we can easily outperform this baseline. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba434531",
   "metadata": {},
   "source": [
    "### MODEL A (codename: Vanilla_Tree) :\n",
    "\n",
    "- Model TRAIN ACCURACY: 81%\n",
    "- Model VALIDATE ACCURACY: 77% slighly OVERFIT\n",
    "- RECALL for YES: 55%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23809b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is a classification report with many different measurements and ratios based upon the confusion matrix\n",
    "# for our purposes here we will look at ACCURACY (0.81) and RECALL for the YES class (0.55).\n",
    "\n",
    "print(\n",
    "    classification_report(y_train,\n",
    "                      model_a_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60d68f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we will evaluate the difference between the ACCURACY score on TRAIN datatset and compare it to the ACCURACY\n",
    "# score on VALIDATE.  If the difference between the two accuracy scores is aproaching 10, then this is an\n",
    "# indication that our model may be OVERFIT...w/o explaining the concept, this is not what we want.  We want a model\n",
    "# that will perform consistently on both TRAIN and VALIDATE.\n",
    "\n",
    "print(f'''\n",
    "Performance in accuracy of Decision Tree 1 on training data:\n",
    "Accuracy (train): {clf.score(X_train, y_train)}\n",
    "Accuracy (validate): {round(clf.score(X_validate, y_validate), 5)}''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f54b2a7",
   "metadata": {},
   "source": [
    "### MODEL B (codename: Barry_Bonds) :\n",
    "\n",
    "- Model ACCURACY: 81% \n",
    "- Model VALIDATE ACCURACY: 77% slighly OVERFIT\n",
    "- RECALL for YES: 59%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a751e59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the Random Forest model \n",
    "rf1 = RandomForestClassifier(n_estimators=25)\n",
    "# fit the model to the TRAIN dataset:\n",
    "rf1.fit(X_train, y_train)\n",
    "# use the model by calling for the predictions made via the TRAIN dataset\n",
    "model_b_preds = rf1.predict(X_train)\n",
    "pd.crosstab(model_b_preds,y_train) # a confusion matrix with ACTUALS as columns and PREDICTIONS as rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473f8a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is a classification report with many different measurements and ratios based upon the confusion matrix\n",
    "# for our purposes here we will look at ACCURACY (0.81) and RECALL for the YES class (0.59).\n",
    "\n",
    "print(\n",
    "    classification_report(y_train,\n",
    "                      model_b_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b54306f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we will evaluate the difference between the ACCURACY score on TRAIN datatset and compare it to the ACCURACY\n",
    "# score on VALIDATE.  If the difference between the two accuracy scores is aproaching 10, then this is an\n",
    "# indication that our model may be OVERFIT...w/o explaining the concept, this is not what we want.  We want a model\n",
    "# that will perform consistently on both TRAIN and VALIDATE.\n",
    "\n",
    "print(f'''\n",
    "Performance in accuracy of Decision Tree 1 on training data:\n",
    "Accuracy (train): {rf1.score(X_train, y_train)}\n",
    "Accuracy (validate): {round(rf1.score(X_validate, y_validate), 5)}''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e068d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0930b01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "230f5eb7",
   "metadata": {},
   "source": [
    "### MODEL C (codename: El_Camino) :\n",
    "\n",
    "\n",
    "- Model ACCURACY: 81% \n",
    "- Model VALIDATE ACCURACY: 77% slighly OVERFIT\n",
    "- RECALL for YES: 58%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd1617e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the Random Forest model \n",
    "rf2 = RandomForestClassifier(n_estimators=101)\n",
    "# fit the model to the TRAIN dataset:\n",
    "rf2.fit(X_train, y_train)\n",
    "# use the model by calling for the predictions made via the TRAIN dataset\n",
    "model_c_preds = rf2.predict(X_train)\n",
    "pd.crosstab(model_c_preds,y_train) # a confusion matrix with ACTUALS as columns and PREDICTIONS as rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a911eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is a classification report with many different measurements and ratios based upon the confusion matrix\n",
    "# for our purposes here we will look at ACCURACY (0.81) and RECALL for the YES class (0.58).\n",
    "\n",
    "print(\n",
    "    classification_report(y_train,\n",
    "                      model_c_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad57c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we will evaluate the difference between the ACCURACY score on TRAIN datatset and compare it to the ACCURACY\n",
    "# score on VALIDATE.  If the difference between the two accuracy scores is aproaching 10, then this is an\n",
    "# indication that our model may be OVERFIT...w/o explaining the concept, this is not what we want.  We want a model\n",
    "# that will perform consistently on both TRAIN and VALIDATE.\n",
    "\n",
    "print(f'''\n",
    "Performance in accuracy of Decision Tree 1 on training data:\n",
    "Accuracy (train): {rf2.score(X_train, y_train)}\n",
    "Accuracy (validate): {round(rf2.score(X_validate, y_validate), 5)}''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c916318e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4523edce",
   "metadata": {},
   "source": [
    "----\n",
    "----\n",
    "----\n",
    "## IN REVIEW:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a3b765",
   "metadata": {},
   "source": [
    "### MODEL A (codename: Vanilla_Tree) :\n",
    "\n",
    "- Model TRAIN ACCURACY: 81%\n",
    "- Model VALIDATE ACCURACY: 77% slighly OVERFIT\n",
    "- RECALL for YES: 55%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d42fbbd",
   "metadata": {},
   "source": [
    "### MODEL B (codename: Barry_Bonds) :\n",
    "\n",
    "- Model ACCURACY: 81% \n",
    "- Model VALIDATE ACCURACY: 77% slighly OVERFIT\n",
    "- RECALL for YES: 59%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbb6760",
   "metadata": {},
   "source": [
    "### MODEL C (codename: El_Camino) :\n",
    "\n",
    "\n",
    "- Model ACCURACY: 81% \n",
    "- Model VALIDATE ACCURACY: 77% slighly OVERFIT\n",
    "- RECALL for YES: 58%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c783b0be",
   "metadata": {},
   "source": [
    "## Currently, the best option is Barry_Bonds due to the Optimized Recall for Yes\n",
    "### This model will identify the LARGEST percentage of Customers with a propensity to Churn at 59% (Hit Rate).  Currently, with a Hit-Rate of 0%, we do not have the option to make any offers to Customers who are at-risk for Churn.  Another way to say it is that we MISS on 100% of all customers who will CHURN.\n",
    "### Barry's Hit-Rate will be able to successfully ID 59% of these Customers and we will actually be in position to make targeted \"Loyalty Offers\" to 59% of this segment of interest.\n",
    "### Given the fact that Barry is an unsophisticated model, in a prototype phase, we will only find BETTER models in the future!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3bb9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "barry_preds = rf1.predict(X_test)\n",
    "barry_proba = rf1.predict_proba(X_test)\n",
    "pd.crosstab(barry_preds,y_test) # a confusion matrix with ACTUALS as columns and PREDICTIONS as rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f7ec2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# here is a classification report with many different measurements and ratios based upon the TEST subset\n",
    "# for our purposes here we will look at ACCURACY (0.76) and RECALL for the YES class (0.45).\n",
    "\n",
    "print(\n",
    "    classification_report(y_test,\n",
    "                      barry_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65f904e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(barry_preds,y_test) # a confusion matrix with ACTUALS as columns and PREDICTIONS as rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0dfb3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "barry_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8918c520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create array of probabilities of being versicolor (versicolor == 1)\n",
    "\n",
    "x_barry_proba = np.array([i[0] for i in barry_proba])\n",
    "y_barry_proba = np.array([i[1] for i in barry_proba])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7c2fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# scatter plot where x is the probabilities and y is the class (0, 1)\n",
    "ax.scatter(y_barry_proba,y_barry_proba,alpha=0.1,color='red',edgecolors='black',s=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70dc997",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98a19767",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
